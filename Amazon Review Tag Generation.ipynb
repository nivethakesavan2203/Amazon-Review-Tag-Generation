{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF,  LatentDirichletAllocation\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import spacy\n",
    "import gzip\n",
    "import simplejson as json\n",
    "import nltk\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "\n",
    "class CleanTextTransformer(TransformerMixin):\n",
    "   \n",
    "    def transform(self, X, **transform_params):\n",
    "        #return [cleanText(text) for text in X]\n",
    "        return [text for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "    \n",
    "\n",
    "def tokenizeText(sample):\n",
    "    \"This function tokenizes text and does other preprocessing steps like Lemmatization and Stemming.\"\n",
    "\n",
    "    tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    #tokenize\n",
    "    tokens = tokenizer.tokenize(sample)\n",
    "    # lemmatize\n",
    "    lemmas = []\n",
    "    for word in tokens:\n",
    "        if word.isalnum() and not word in stop_words:\n",
    "            word = word.lower()\n",
    "            word = lemmatizer.lemmatize(word, pos = 'v')\n",
    "            lemmas.append(word)\n",
    "    tokens = lemmas\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def return_topics(vectorizer, clf, W, df, n_top_words, n_top_documents):\n",
    "    topics, reviews = [], []\n",
    "    features = vectorizer.get_feature_names()\n",
    "    sentiment_analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "    for topic_id, topic in enumerate(clf.components_):\n",
    "\n",
    "        # grab the list of words describing the topic\n",
    "        topic_word_list = []\n",
    "        for i in topic.argsort()[:-n_top_words - 1:-1]:\n",
    "            topic_word_list.append(features[i])\n",
    "\n",
    "        # split words in case there are some bigrams\n",
    "        split_topic_word_list = []\n",
    "        for word in topic_word_list:\n",
    "            for splitted in word.split():\n",
    "                split_topic_word_list.append(splitted)\n",
    "        topic_words = list(set(split_topic_word_list))\n",
    "\n",
    "        # append topic words as a single string\n",
    "        topics.append(' '.join([word for word in topic_words]))\n",
    "\n",
    "        # iterate for reviews for each topic\n",
    "        topic_doc_indices = np.argsort(W[:, topic_id])[::-1][0:n_top_documents]\n",
    "\n",
    "        for doc_ind in topic_doc_indices:\n",
    "            review = df['reviewText'].iloc[doc_ind]\n",
    "\n",
    "            # check if the review contains any of the topic words\n",
    "            if any(word in review.lower() for word in topic_words):\n",
    "                # analyse sentiment\n",
    "                vader = sentiment_analyser.polarity_scores(review)\n",
    "                # form the review - topic_id and sentiment data structure\n",
    "                reviews.append(df.iloc[doc_ind].to_dict())\n",
    "                reviews[-1]['topic'] = topic_id\n",
    "                reviews[-1]['sentiment'] = vader['compound']\n",
    "\n",
    "    return topics, reviews\n",
    "\n",
    "\n",
    "def summarize_reviews(topics, reviews):\n",
    "    # returns reviews with the following new fields\n",
    "    #  'summary': sentences from review w/ topic words\n",
    "\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    summary_all_review = []\n",
    "    for ii, review in enumerate(reviews):\n",
    "        summary = []\n",
    "        sentences = sent_tokenize(review['reviewText'])\n",
    "        topic_words = topics[review['topic']].split()\n",
    "\n",
    "        for sentence in sentences:\n",
    "            for word in topic_words:\n",
    "                if word in sentence.lower():\n",
    "                    summary.append(sentence)\n",
    "                    break\n",
    "\n",
    "        reviews[ii]['summary'] = ' '.join([sentence for sentence in summary])\n",
    "        vader = analyser.polarity_scores(reviews[ii]['summary'])\n",
    "        reviews[ii]['summary_sentiment'] = vader['compound']\n",
    "        \n",
    "        summary_all_review.append(reviews[ii]['summary'])\n",
    "\n",
    "    return reviews, summary_all_review\n",
    "\n",
    "def print_topics(test_asin):\n",
    "\n",
    "    test_df = reviews_df[reviews_df['asin'] == test_asin].dropna()\n",
    "    n_features, n_top_words, n_topics, n_top_documents = 1000, 3, 13, 3\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=n_features,\n",
    "                                 tokenizer=tokenizeText,\n",
    "                                 stop_words='english',\n",
    "                                 ngram_range=(1,2))\n",
    "\n",
    "    clf = NMF(n_components=n_topics, random_state=1, solver='mu', beta_loss='frobenius')\n",
    "   \n",
    "    #clf = LatentDirichletAllocation(n_components = 5, max_iter = 5, learning_method ='online',learning_offset = 50.,random_state = 0)\n",
    "\n",
    "    pipe = Pipeline([('cleanText', CleanTextTransformer()),('vectorizer', vectorizer), ('nmf', clf)])\n",
    "\n",
    "    pipe.fit(test_df['reviewText'])\n",
    "    transform = pipe.fit_transform(test_df['reviewText'])\n",
    "    \n",
    "    #topic identification\n",
    "    topics, reviews = return_topics(vectorizer, clf, transform, test_df, n_top_words, n_top_documents)\n",
    "    # review summarization\n",
    "    reviews , summary = summarize_reviews(topics, reviews)\n",
    "    print(\"Topics:\", len(topics))\n",
    "    \n",
    "    return topics, reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       reviewerID        asin              reviewerName   helpful  \\\n",
      "0   AO94DHGC771SJ  0528881469                   amazdnu    [0, 0]   \n",
      "1   AMO214LNFCEI4  0528881469           Amazon Customer  [12, 15]   \n",
      "2  A3N7T0DY83Y4IG  0528881469             C. A. Freeman  [43, 45]   \n",
      "3  A1H8PY3QHMQQA0  0528881469  Dave M. Shaw \"mack dave\"   [9, 10]   \n",
      "\n",
      "                                          reviewText  overall  \\\n",
      "0  We got this GPS for my husband who is an (OTR)...      5.0   \n",
      "1  I'm a professional OTR truck driver, and I bou...      1.0   \n",
      "2  Well, what can I say.  I've had this unit in m...      3.0   \n",
      "3  Not going to write a long review, even thought...      2.0   \n",
      "\n",
      "                   summary  unixReviewTime   reviewTime  \n",
      "0          Gotta have GPS!      1370131200   06 2, 2013  \n",
      "1        Very Disappointed      1290643200  11 25, 2010  \n",
      "2           1st impression      1283990400   09 9, 2010  \n",
      "3  Great grafics, POOR GPS      1290556800  11 24, 2010  \n"
     ]
    }
   ],
   "source": [
    "#reviews_df = getDF('Video_Games_5.json.gz')\n",
    "reviews_df = getDF('Electronics_5.json.gz')\n",
    "print(reviews_df.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_entropy(n):\n",
    "    return -np.log(1/n)\n",
    "\n",
    "def unique(sequence):\n",
    "    '''get unique elements of list and keep the same order'''\n",
    "    \n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "\n",
    "def redundancy(string):\n",
    "    entropy, string_list = 0, string.split()\n",
    "    string_set = unique(string_list)\n",
    "    for word in string_set:\n",
    "        p = string_list.count(word)/len(string_list)\n",
    "        entropy -= p*np.log(p)        \n",
    "    return 1 - entropy/max_entropy(len(string_list))\n",
    "\n",
    "def lemmatize(string):\n",
    "    from nltk.stem import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = parser(string)\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        lemmas.append(stemmer.stem(token.lemma_.lower().strip()))\n",
    "        \n",
    "    return ' '.join(lemma for lemma in lemmas)\n",
    "\n",
    "def adjective_count(lemmas):\n",
    "    string = nltk.word_tokenize(lemmas)\n",
    "    pos_string = nltk.pos_tag(string)\n",
    "    count = 0\n",
    "    for i in pos_string:\n",
    "        if i[1] == 'JJ' or i[1] == 'JJR' or i[1] == 'JJS' or i[1] == 'RB' or i[1] == 'RBR' or i[1] == 'RBS':\n",
    "            count += 1\n",
    "    return count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker system sound quality headphone jack satellite speakers sound card highly recommend thx certify midrange bang buck live room home theater altec lansing\n",
      "0.019223019952826492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "['bass loud volume', 'z 2300', 'sound great', 'speakers computer', 'cable remote satellite', 'good sound', 'stop work great', 'speaker best bose', 'review beat price', 'play sound card', 'music movies watch', 'use easy quality', 'buy worth excellent']\n",
      "bass loud volume z 2300 sound great speakers computer cable remote satellite good sound stop work great speaker best bose review beat price play sound card music movies watch use easy quality buy worth excellent\n",
      "0.03762646458355279\n"
     ]
    }
   ],
   "source": [
    "string = 'speaker system sound quality headphone jack satellite speakers sound card highly recommend thx certified midrange bang for the buck living room home theater altec lansing'\n",
    "lemmas = tokenizeText(string) \n",
    "lemmas = ' '.join(lemma for lemma in lemmas)\n",
    "print(lemmas)\n",
    "print(redundancy(lemmas))\n",
    "\n",
    "topics, reviews = print_topics('B0002SQ2P2')\n",
    "print(topics)\n",
    "lemmas = ' '.join(lemma for lemma in topics)\n",
    "print(lemmas)\n",
    "print(redundancy(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earbuds sound quality couple months every 6 months sound good great sound bass volume ears earbud earphones hear inexpensive jack model break cord\n",
      "0.06492466861659307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "['music sound headphones', 'studio years home', 'ears head wear', 'sound comfortable great', 'review headphones say', 'response frequency price', 'work wear hours', 'fit large amp', 'bass monitor bite', 'audio technica', 'love far im', 'overall good better', 'hop record edit']\n",
      "music sound headphones studio years home ears head wear sound comfortable great review headphones say response frequency price work wear hours fit large amp bass monitor bite audio technica love far im overall good better hop record edit\n",
      "0.030087065120016687\n"
     ]
    }
   ],
   "source": [
    "string = 'earbuds sound quality couple months every 6 months sound was good great sound bass volume ears earbud earphones hear inexpensive jack model broke cord'\n",
    "lemmas = tokenizeText(string) \n",
    "lemmas = ' '.join(lemma for lemma in lemmas)\n",
    "print(lemmas)\n",
    "print(redundancy(lemmas))\n",
    "\n",
    "\n",
    "\n",
    "topics, review = print_topics('B0002D03ZW')\n",
    "print(topics)\n",
    "lemmas = ' '.join(lemma for lemma in topics)\n",
    "print(lemmas)\n",
    "print(redundancy(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def grid_search_topic_model(test_asin):\n",
    "    search_params = {'n_components': [3,4,5,6,7,8,9,10,11,12]}\n",
    "    test_df = reviews_df[reviews_df['asin'] == test_asin].dropna()\n",
    "    n_features, n_top_words, n_topics, n_top_documents = 1000, 3, 6, 3\n",
    "    vectorizer = TfidfVectorizer(max_features=n_features,\n",
    "                                 tokenizer=tokenizeText,\n",
    "                                 stop_words='english',\n",
    "                                 ngram_range=(1,2))\n",
    "\n",
    "    clf = NMF()\n",
    "    clf = LatentDirichletAllocation()\n",
    "\n",
    "    pipe = Pipeline([('cleanText', CleanTextTransformer()),('vectorizer', vectorizer)])\n",
    "\n",
    "    # pipe.fit(test_df['reviewText'])\n",
    "    data_vectorized = pipe.fit_transform(test_df['reviewText'])\n",
    "\n",
    "    model = GridSearchCV(clf, param_grid=search_params)\n",
    "\n",
    "    model.fit(data_vectorized)\n",
    "    best_topic_model = model.best_estimator_\n",
    "    print(\"Best Model's Params: \", model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model's Params:  {'n_components': 3}\n"
     ]
    }
   ],
   "source": [
    "grid_search_topic_model('B0002SQ2P2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics_lda(test_asin):\n",
    "\n",
    "    test_df = reviews_df[reviews_df['asin'] == test_asin].dropna()\n",
    "    n_features, n_top_words, n_topics, n_top_documents = 1000, 3, 13, 3\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=n_features,\n",
    "                                 tokenizer=tokenizeText,\n",
    "                                 stop_words='english',\n",
    "                                 ngram_range=(1,2))\n",
    "\n",
    "    clf = NMF(n_components=n_topics, random_state=1, solver='mu', beta_loss='frobenius')\n",
    "   \n",
    "    clf = LatentDirichletAllocation(n_components = 3, max_iter = 5, learning_method ='online',learning_offset = 50.,random_state = 0)\n",
    "\n",
    "    pipe = Pipeline([('cleanText', CleanTextTransformer()),('vectorizer', vectorizer), ('nmf', clf)])\n",
    "\n",
    "    pipe.fit(test_df['reviewText'])\n",
    "    transform = pipe.fit_transform(test_df['reviewText'])\n",
    "    \n",
    "    #topic identification\n",
    "    topics, reviews = return_topics(vectorizer, clf, transform, test_df, n_top_words, n_top_documents)\n",
    "    # review summarization\n",
    "    summary = summarize_reviews(topics, reviews)\n",
    "    #print(\"Summary :\\n\", summary)\n",
    "    print(\"Topics:\")\n",
    "    \n",
    "    return topics, reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker system sound quality headphone jack satellite speakers sound card highly recommend thx certify midrange bang buck live room home theater altec lansing\n",
      "0.019223019952826492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "['speakers like better', 'speakers sound great', 'speakers sound bass']\n",
      "speakers like better speakers sound great speakers sound bass\n",
      "0.23676997261905086\n"
     ]
    }
   ],
   "source": [
    "string = 'speaker system sound quality headphone jack satellite speakers sound card highly recommend thx certified midrange bang for the buck living room home theater altec lansing'\n",
    "lemmas = tokenizeText(string) \n",
    "lemmas = ' '.join(lemma for lemma in lemmas)\n",
    "print(lemmas)\n",
    "print(redundancy(lemmas))\n",
    "\n",
    "\n",
    "\n",
    "topics, review = print_topics_lda('B0002SQ2P2')\n",
    "print(topics)\n",
    "lemmas = ' '.join(lemma for lemma in topics)\n",
    "print(lemmas)\n",
    "print(redundancy(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0528881469', '0528881469', '0528881469', '0528881469', '0528881469']\n",
      "['B00066EK2W', 'B0002D03ZW', 'B00004TS16', 'B0001NNLHK', 'B0002IOIMQ', 'B0000DJEK7', 'B0002UPGOI', 'B00028D778', 'B00005QXWI', 'B0002CZHN6', 'B0000AQIFW', 'B0002Y5WXO', 'B00005NIMJ', 'B00007GQLU', 'B00062UW5A', 'B00017LSPI', 'B00066HP7Y', 'B000629GES', 'B00009WQS1', 'B0002KVQBA', 'B00030CHRQ', 'B00020S7XK', 'B0000BYDKO', 'B00007M1TZ', 'B00018MSNI', 'B0002UM0JW', 'B0002SQ2P2', 'B00004T8R2', 'B0000C3GWU', 'B000204SWE', 'B00006JN3G', 'B0002WPSBC']\n",
      "['B00066EK2W', 'B0002D03ZW', 'B00004TS16', 'B0001NNLHK', 'B0002IOIMQ', 'B0000DJEK7', 'B0002UPGOI', 'B00028D778', 'B00005QXWI', 'B0002CZHN6', 'B0000AQIFW', 'B0002Y5WXO', 'B00005NIMJ', 'B00007GQLU', 'B00062UW5A', 'B00017LSPI', 'B00066HP7Y', 'B000629GES', 'B00009WQS1', 'B0002KVQBA', 'B00030CHRQ', 'B00020S7XK', 'B0000BYDKO', 'B00007M1TZ', 'B00018MSNI', 'B0002UM0JW', 'B0002SQ2P2', 'B00004T8R2', 'B0000C3GWU', 'B000204SWE', 'B00006JN3G', 'B0002WPSBC']\n"
     ]
    }
   ],
   "source": [
    "test_amazon_asins = ['B0002SQ2P2', 'B0002KVQBA', 'B00029MTMQ','B00020S7XK','B00063E2HS','B0002D03ZW','B0002WPSBC','B00006JN3G','B0002CZHN6','B00004T8R2','B00004ZCJJ',\n",
    "'B00007M1TZ','B0002UPGOI','B000204SWE','B0002EMY9Y','B00006IAKJ','B000629GES','B00017LSPI','B0002UM0JW',\n",
    "'B0000C3GWU','B0001NNLHK','B0000BYDKO','B00008MOPJ','B00066HP7Y','B0000AQIFW','B00066EK2W','B00005NIMJ',\n",
    "'B00009WQS1','B0000DJEK7','B00028D778','B00030CHRQ','B0002IOIMQ','B0001EMA80','B00006JILE','B0002Y5WXO',\n",
    "'B00062UW5A','B00007GQLU','B00004TS16','B00005QXWI','B00018MSNI']\n",
    "\n",
    "test_elecronics5_asins = reviews_df['asin']\n",
    "print(list(test_elecronics5_asins[:5]))\n",
    "\n",
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2))\n",
    "\n",
    "print(intersection(test_amazon_asins, test_elecronics5_asins)) \n",
    "\n",
    "valid_asins = intersection(test_amazon_asins, test_elecronics5_asins)\n",
    "\n",
    "print(valid_asins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B00066EK2W ['player sandisk good', 'use easy', 'drive hard', 'memory expandable mp3', 'file crackle wma', 'amaze sweat years', 'include usb mp3', 'button buy small', 'music classical solve volume', 'operation gift basic recommend product', 'point good review product', 'small ear easy v', 'audio track record']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B0002D03ZW ['music sound headphones', 'studio years home', 'ears head wear', 'sound comfortable great', 'review headphones say', 'response frequency price', 'work wear hours', 'fit large amp', 'bass monitor bite', 'audio technica', 'love far im', 'overall good better', 'hop record edit']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B00004TS16 ['shoot shots picture', 'thing like memory card', 'paper machine quality', 'photography camera snorkel', 'manual nice mode', 'think overall camera great', 's100 fancy friends', 'thing usb easy', 'battery backup photoshop', 'bother camera 2mp', 'tft picture deal', 'battery lithium charger', 'reader small pocket goodbye']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B0001NNLHK ['sound good sleeves', 'ear bud e3c', 'ears cord headphones', 'ipod listen earphones', 'break wire cover', 'love fit sound supply', 'star price general', 'canal ear send', 'player e3cs purchase', 'tip foam earplugs', 'bass koss 10', 'hear long flight', 'noise sound block']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B0002IOIMQ ['batteries charge charger', 'display lcd', 'use years work', 'battery life', 'super quick', 'batteries days sony', 'sony cells kill', 'nice charge batteries', 'battery know let charge 4', 'fast charger', 'camera digital', 'batteries faster great', 'outlet plug wall']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B0000DJEK7 ['use unit roadmate', 'warranty extend', 'update ask customer', 'battery internal', 'location save device', 'loose fiancee door', 'disappoint view excellent', 'use freeway wife', 'newer sync gps', '2004 speak model', 'destination lexus cities', 'minutes time months 10 5', 'north interface america']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B0002UPGOI ['speaker use wire', 'need install amp', 'cheap work great quality', 'watts rms', 'ship fast price amp', 'thing watt price great', 'fine diameter work', 'lead grind truck power', 'make clean easy amp', 'plenty wish wire', 'product maybe u', 'wont work say', 'help ice build']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B00028D778 ['radar detector', 'cord unit passport', 'time cop drive', 'service year heat', '40 display cost', 'expensive know protection', 'mount suction windshield', 'better years old', 'escort product website', 'relatively false positives car', 'item recommend seller', 'x50 band units', 'far trust totally']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B00005QXWI ['turn cd mp3', 'screw plastic work', 'good really sony', 'use prevent trouble', 'player mp3 rangeit far function', '1 expensive 3', 'player qualities purpose', 'listen walk book', '250 100 90', 'use optional disk accessory', 'riovolt provide feature', 'skip intuitive protection quite impressive', 'hope good qualitycons rat']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B0002CZHN6 ['cable hdmi dvi', 'cable price work', 'laptop computer monitor', 'good quality', 'work great', 'pc tv connect', 'xbox 360', 'perfect work monitor', 'audio carry', 'use ps3 problems', 'review work product', 'video card', 'buy long quality']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B0000AQIFW ['player mp3 songs', 'use easy stop product', 'year hours pretty', 'player sound headphones', 'ergonomic better software device', '40 perfect expandable', 'know sonic product blue', 'practical unit version 256meg', 'music gb hours cali', 'useless love send', 'rip cd', 'mp3s computer car hell', 'upgrader rio save']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B0002Y5WXO ['lens kit', 'light stabilization zoom', 'print mm lense', 'mm tamron canon', 'repair canon problem', 'lens mm', 'shift return amazon', 'heavy picture big', '20d nice eos work great', 'lens money really', 'purpose general', 'price need worth f4l', 'lens good crisp']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B00005NIMJ ['use regular mouse', 'trackman marble wheel', 'version wire wireless', 'track ball', 'button easy trackball', 'surface use', '5 years logitech', 'button thumb finger', 'love home game', 'laptop use work great', 'like feel model', 'tunnel carpal great', 'cord cordless device']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B00007GQLU ['lens use shoot', 'f fringe l', 'f1 8', 'lens love', 'lens l purchase', 'nice image crisp', 'light low', 'lens portraits great', 'crop sensor', 'lens lenses make', 'sharp best fast', '85mm ef canon', 'lens like recommend']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B00062UW5A ['box jewel case', 'paper sleeves', 'need storage look', 'store really easy organize', 'easy assemble', 'fold need store', 'box snap lid', 'box dvd purchase', 'love use apart', 'snap stay', 'handy price product quality', 'box work', 'collection cd']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B00017LSPI ['dust blow', 'air compress', 'work great', 'bag camera', 'clean sensor', 'air rocket blaster', 'blower nice quality', 'lens dust remove', 'use remove dust', 'air blast', 'buy good product', 'large work size', 'like look']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\nmf.py:1069: ConvergenceWarning: Maximum number of iteration 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\nmf.py:1069: ConvergenceWarning: Maximum number of iteration 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B00066HP7Y ['buy pc car', 'thing drum trick', 'thing ipod brain say', 'sound great', 'complaints brain electronic', 'sure shield bite static work', 'cable audio headphones', 'cable audio quality', 'quality remote great', 'sound distortion good', 'cable sound defective', 'suppose computer tv flat', 'quite far bite static']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B000629GES ['cancel noise', 'sony headphones great', 'good bose', 'ears uncomfortable flight', 'advertise work fragile', 'return good pair', 'reduce buy product', 'noise cancellation sound', 'think dollar loud', 'pad ear', 'recommend hiss background headphones', 'ones power flight', 'order sony radio']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B00009WQS1 ['drive hard', 'support version work', 'external usb drive', 'use ghost enclosure', 'right box', 'hdd computer dell', 'laptop upgrade ez', 'way install connect', 'ssd minutes drive', 'like champ', 'latitude time dell', 'recovery space partition', 'recommend pc highly products']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B0002KVQBA ['bass sound good', '10 psw', 'better sub want', 'sound great', 'subwoofer input rca', 'live room', 'theater home', 'price product quality', 'port noise', 'speakers bookshelf polk', 'music movies want', 'nice unit power', 'buy woofer surround']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B00030CHRQ ['cord ipod headphones', 'buy sound earbuds', 'shure headphones quality', 'coat wire expose', 'bass buy sony', 'long extension size', 'use price', 'cable ear leave', 'house comfortably ear', 'ears 5 completely', 'bag music hear', 'run say great', 'player mp3']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B00020S7XK ['fm station radio', 'price little great', 'fine work', 'batteries aa use', 'sound good quality', 'mono headphones stereo', 'good radio little', 'control tune volume', 'baseball listen game', 'battery life', 'sony radio pocket', 'buy love clear', 'emergency case power']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B0000BYDKO ['cord reel', 'cable weight reel', 'hole end plug', 'tie half fold', 'hose use reel', 'buy product great', 'heavy cord gauge', 'use cord time', 'foot cord 100', 'cord extension 50', 'cord hold extention', 'like really look', 'chord thrower bite']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B00007M1TZ ['use headsets headset', 'phone cordless', 'sound quality', 'people hear clearly', 'price work great', 'control volume', 'fit head set', 'wear comfortable', 'free hand', 'buy mm jack', 'work headset', 'use home phone', 'break expect months']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B00018MSNI ['like hd650 headphones', 'hd 650', 'sound hear headphones', 'headphone need sound', 'tube hd600 amp', 'ago years', 'hd650 sound grado', 'watch tv', '650s hd', 'twice phone phenomenal', 'sound amplifier heck', 'dac end high', 'm100 wear design']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B0002UM0JW ['use mouse optical', 'mx1000 surface mx700', 'button forward', 'months close click', 'ergonomics w good', 'mx revolution', 'mac easy set', 'logitech mouse great', 'years mouse time', 'jump time long', 'charge anymore usage', 'office home', 'use original actually']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B0002SQ2P2 ['bass loud volume', 'z 2300', 'sound great', 'speakers computer', 'cable remote satellite', 'good sound', 'stop work great', 'speaker best bose', 'review beat price', 'play sound card', 'music movies watch', 'use easy quality', 'buy worth excellent']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B00004T8R2 ['sound pair headphones', 'good sound', 'price work great', 'cord use long', 'recommend highly', 'fit head phone', 'price low volume', 'buy pair', 'port lightweight xbs', 'use kid want', 'player mp3', 'ear bud', 'light weight']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B0000C3GWU ['speakers sound quality', 'control turn volume', 'work great', 'use sound great', 'book mac pro sound', 'save feature second', 'bass recommend highly', 'amaze pay dollars', 'harman kardon', 'happy purchase', 'phone cell', 'speakers sound set', 'player cd']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B000204SWE ['player dvd', 'play divx file', 'unit turn region', 'dvds play', 'philips warranty months', 'price work great', 'button hold stop close', 'dvp 642', 'picture quality', 'use composite component', 'mpeg 4', 'player remote work media', 'play good model']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B00006JN3G ['lens pen', 'bag camera', 'lenses clean', 'dust brush end', 'use easy carry', 'work product great', 'lens clean', 'good job', 'use black time', 'lens dust filter', 'cheap clean tool', 'kit clean camera', 'handy buy friend']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "B0002WPSBC ['speakers use speaker', 'z 5500', 'speakers price set', 'sound great', 'day years ago', 'theater home', 'break customer send', 'sound card', 'sound loud quality', 'amaze buy clear', 'promedia klipsch', 'music listen game', 'recommend control highly panel']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B00066EK2W ['player use mp3', 'battery issue rechargeable radio', 'player use mp3']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B0002D03ZW ['use sound headphones', 'headphone better record', 'break sound warn']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B00004TS16 ['battery camera picture', 'recommend realize size', 'fantastically camera produce zoom']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B0001NNLHK ['listen sound hear', 'shure rubber music', 'shure earphones headphones']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B0002IOIMQ ['batteries time problem', 'batteries charge charger', 'batteries charge charger']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B0000DJEK7 ['unit gps roadmate', 'right probably unit car', 'use model great']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B0002UPGOI ['price work inexpensive solution', 'box safely speaker wire way', 'need cut excellent say']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B00028D778 ['radar item detector', 'right x50 claim smart', 'unit need years']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B00005QXWI ['player year button', 'recharge leather read manuals', 'player make produce screw']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B0002CZHN6 ['cable hdmi dvi', 'cable work great', 'connect work product']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B0000AQIFW ['iriver color device', 'player good mp3', 'sonic look product']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B0002Y5WXO ['lens include lense', 'mm shoot need lense', 'lens good canon']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B00005NIMJ ['trackman use mouse', 'use delivery mouse', 'use mouse ball']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B00007GQLU ['lens prime price great', 'lens reasonable great', 'lens f great']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B00062UW5A ['box cd great', 'box snap use', 'use storage case']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B00017LSPI ['squeeze work blow', 'manual pfff bring', 'dust air use']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B00066HP7Y ['cable flat screen', 'garbage need piece leave', 'far pc car stream']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B000629GES ['earphones work noise', 'really headphones bose', 'know good headphones ok']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B00009WQS1 ['use hd drive', 'use drive hard', 'clone drive hard']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B0002KVQBA ['subwoofer sub sound', 'repair sub room', 'huummm subwoofer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B00030CHRQ ['cord sound sony', 'cord buy use', 'cord ear design']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B00020S7XK ['whistle work radio', 'today stop radio order', 'good radio great']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B0000BYDKO ['cord use reel', 'cord use reel', 'thing cord junk']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B00007M1TZ ['cable hear say', 'use hear clearly', 'use phone headset']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B00018MSNI ['headphone sound headphones', 'replace close headphones', 'like sound headphones']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B0002UM0JW ['mouse hours months', 'button use mouse', 'charge good mouse']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B0002SQ2P2 ['speakers like better', 'speakers sound great', 'speakers sound bass']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B00004T8R2 ['use sound headphones', 'bargain wear comfortable note', 'sound price headphones']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B0000C3GWU ['speakers sound price', 'speakers compact', 'speakers imac work']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B000204SWE ['play europe dvds', 'player play dvd', 'player gift philips']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B00006JN3G ['lens use clean', 'lenses clean carry', 'lenspen brush make wear']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "B0002WPSBC ['tigerdirect price sound', 'speakers use sound', 'speakers sound great']\n"
     ]
    }
   ],
   "source": [
    "red_nmf = 0 \n",
    "adj_count_lda, adj_count_nmf = [], []\n",
    "\n",
    "redundancy_arr = ()\n",
    "\n",
    "for asin in valid_asins:\n",
    "    topics, review = print_topics(asin)\n",
    "    print(asin,topics)\n",
    "    lemmas = ' '.join(lemma for lemma in topics)\n",
    "    temp_red = redundancy(lemmas)\n",
    "    red_nmf += temp_red\n",
    "    redundancy_arr = list(redundancy_arr)\n",
    "    redundancy_arr.append((asin,temp_red))\n",
    "    redundancy_arr = tuple(redundancy_arr)\n",
    "    \n",
    "    count = 0\n",
    "    count = adjective_count(lemmas) / len(topics)\n",
    "    adj_count_nmf.append(count)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "red_lda = 0 \n",
    "for asin in valid_asins:\n",
    "    topics, review = print_topics_lda(asin)\n",
    "    print(asin, topics)\n",
    "    lemmas = ' '.join(lemma for lemma in topics)\n",
    "    red_lda += redundancy(lemmas)\n",
    "    \n",
    "    count = 0\n",
    "    count = adjective_count(lemmas) / len(topics)\n",
    "    adj_count_lda.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('B00066EK2W', 0.044154529384612284), ('B0002D03ZW', 0.030087065120016687), ('B00004TS16', 0.04748769962424759), ('B0001NNLHK', 0.03173139218403054), ('B0002IOIMQ', 0.1007691788830547), ('B0000DJEK7', 0.009702582539327653), ('B0002UPGOI', 0.06647158925456631), ('B00028D778', 1.1102230246251565e-15), ('B00005QXWI', 0.03237118230576741), ('B0002CZHN6', 0.0809371426841029), ('B0000AQIFW', 0.024977625124585345), ('B0002Y5WXO', 0.07158025692411119), ('B00005NIMJ', 0.04390147839933556), ('B00007GQLU', 0.09753396024068128), ('B00062UW5A', 0.08596729049656449), ('B00017LSPI', 0.08443609377704353), ('B00066HP7Y', 0.10075788379386086), ('B000629GES', 0.05188073513690761), ('B00009WQS1', 0.03504485293793502), ('B0002KVQBA', 0.02312489790974337), ('B00030CHRQ', 0.03112844108214441), ('B00020S7XK', 0.045420999965316566), ('B0000BYDKO', 0.11164639178466218), ('B00007M1TZ', 0.04805814864740876), ('B00018MSNI', 0.07798360875751442), ('B0002UM0JW', 0.04703962232270176), ('B0002SQ2P2', 0.03762646458355279), ('B00004T8R2', 0.04805814864740876), ('B0000C3GWU', 0.06684309322072657), ('B000204SWE', 0.04703962232270176), ('B00006JN3G', 0.10842621469906477), ('B0002WPSBC', 0.03762646458355279))\n"
     ]
    }
   ],
   "source": [
    "print(redundancy_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg reduncdancy NMF: 0.055306708041789096\n",
      "Avg reduncdancy LDA: 0.14086402562112305\n",
      "Avg Adjective and Adverb Count NMF: 0.7740384615384617\n",
      "Avg Adjective and Adverb Count LDA: 0.5937500000000001\n"
     ]
    }
   ],
   "source": [
    "asins_count = len(valid_asins)\n",
    "\n",
    "avd_red_nmf = red_nmf/asins_count\n",
    "avg_red_lda = red_lda/asins_count\n",
    "\n",
    "print('Avg reduncdancy NMF:', avd_red_nmf)\n",
    "print('Avg reduncdancy LDA:', avg_red_lda)\n",
    "\n",
    "\n",
    "avg_adj_count_nmf = sum(adj_count_nmf) / asins_count\n",
    "avg_adj_count_lda = sum(adj_count_lda) / asins_count\n",
    "\n",
    "print('Avg Adjective and Adverb Count NMF:', avg_adj_count_nmf)\n",
    "print('Avg Adjective and Adverb Count LDA:', avg_adj_count_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_tags_df = pd.read_csv('amazon_scraped_tags.csv',keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Product', '# of Reviews', 'ASIN', 'Tags', 'Unnamed: 4', 'Unnamed: 5',\n",
      "       'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10',\n",
      "       'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14',\n",
      "       'Unnamed: 15', 'Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18',\n",
      "       'Unnamed: 19'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASIN</th>\n",
       "      <th>CombinedTags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B0002SQ2P2</td>\n",
       "      <td>speaker system sound quality headphone jack sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0002KVQBA</td>\n",
       "      <td>polk audio living room sounds great surround s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00020S7XK</td>\n",
       "      <td>great little pocket radio battery life sound q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B0002D03ZW</td>\n",
       "      <td>earbuds sound quality couple months every 6 mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B0002WPSBC</td>\n",
       "      <td>home theater surround sound speaker system sou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ASIN                                       CombinedTags\n",
       "0  B0002SQ2P2  speaker system sound quality headphone jack sa...\n",
       "1  B0002KVQBA  polk audio living room sounds great surround s...\n",
       "3  B00020S7XK  great little pocket radio battery life sound q...\n",
       "5  B0002D03ZW  earbuds sound quality couple months every 6 mo...\n",
       "6  B0002WPSBC  home theater surround sound speaker system sou..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(scraped_tags_df.columns)\n",
    "\n",
    "tags_df = scraped_tags_df.iloc[: , 2 : 20]\n",
    "tags_df.head(5)\n",
    "\n",
    "tags_df['CombinedTags'] = tags_df[tags_df.columns[1:]].apply(\n",
    "    lambda x: ' '.join(x.dropna().astype(str)),\n",
    "    axis=1\n",
    ")\n",
    "asin_tags_df = tags_df.loc[ : , ['ASIN','CombinedTags'] ]\n",
    "asin_tags_df= asin_tags_df[asin_tags_df.ASIN.isin(valid_asins)]\n",
    "asin_tags_df.to_csv('valid_asin_scraped_tags_df.csv')\n",
    "\n",
    "asin_tags_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B0002SQ2P2', 'B0002KVQBA', 'B00020S7XK', 'B0002D03ZW', 'B0002WPSBC', 'B00006JN3G', 'B0002CZHN6', 'B00004T8R2', 'B00007M1TZ', 'B0002UPGOI', 'B000204SWE', 'B000629GES', 'B00017LSPI', 'B0002UM0JW', 'B0000C3GWU', 'B0001NNLHK', 'B0000BYDKO', 'B00066HP7Y', 'B0000AQIFW', 'B00066EK2W', 'B00005NIMJ', 'B00009WQS1', 'B0000DJEK7', 'B00028D778', 'B00030CHRQ', 'B0002IOIMQ', 'B0002Y5WXO', 'B00062UW5A', 'B00007GQLU', 'B00004TS16', 'B00005QXWI', 'B00018MSNI']\n",
      "Avg reduncdancy NMF: 0.06690723690841899\n"
     ]
    }
   ],
   "source": [
    "valid_asins = list(asin_tags_df['ASIN'])\n",
    "print(valid_asins)\n",
    "review_sentences = list(asin_tags_df['CombinedTags'])\n",
    "redundancy_arr = ()\n",
    "\n",
    "red_amazon_scraped_tags, adj_count = 0, 0 \n",
    "for i in range(len(review_sentences)):\n",
    "    temp_red = redundancy(review_sentences[i])\n",
    "    red_amazon_scraped_tags += temp_red\n",
    "    redundancy_arr = list(redundancy_arr)\n",
    "    redundancy_arr.append((valid_asins[i],temp_red))\n",
    "    redundancy_arr = tuple(redundancy_arr)\n",
    "    adj_count += (adjective_count(review_sentences[i])/13)\n",
    "    \n",
    "asins_count = len(valid_asins)\n",
    "\n",
    "avd_red_amazon_scraped_tags = red_amazon_scraped_tags/asins_count\n",
    "\n",
    "print('Avg reduncdancy NMF:', avd_red_amazon_scraped_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(asins_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('B0002SQ2P2', 0.01722706232293625), ('B0002KVQBA', 0.07407407407407396), ('B00020S7XK', 0.05678533309253431), ('B0002D03ZW', 0.06138624817088656), ('B0002WPSBC', 0.12073246487342737), ('B00006JN3G', 0.11009096747637015), ('B0002CZHN6', 0.044574556644966234), ('B00004T8R2', 0.0837725743386264), ('B00007M1TZ', 0.032730162085132464), ('B0002UPGOI', 0.05000000000000038), ('B000204SWE', 0.014196333273133965), ('B000629GES', 0.15912321167269505), ('B00017LSPI', 0.132598753895238), ('B0002UM0JW', 0.12080961137566348), ('B0000C3GWU', 0.06546032417026526), ('B0001NNLHK', 0.03273016208513235), ('B0000BYDKO', 0.042588999819400786), ('B00066HP7Y', 0.09693609377704371), ('B0000AQIFW', 0.03273016208513235), ('B00066EK2W', 0.04075900941810162), ('B00005NIMJ', 0.029716371096644156), ('B00009WQS1', 0.11381876958331827), ('B0000DJEK7', 6.661338147750939e-16), ('B00028D778', 0.01722706232293625), ('B00030CHRQ', 0.11886548438657696), ('B0002IOIMQ', 0.09263755863796663), ('B0002Y5WXO', 0.06819406190476329), ('B00062UW5A', 0.08664609663791745), ('B00007GQLU', 0.03273016208513235), ('B00004TS16', 0.09053631692101882), ('B00005QXWI', 0.029716371096644156), ('B00018MSNI', 0.07163722174572817))\n"
     ]
    }
   ],
   "source": [
    "print(redundancy_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average adjective and Adverb count in Scraped Tags 0.5697115384615384\n"
     ]
    }
   ],
   "source": [
    "print(\"Average adjective and Adverb count in Scraped Tags\", adj_count/asins_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "😐 - bass loud volume\n",
      "🙂 - z 2300\n",
      "🙂 - sound great\n",
      "🙂 - speakers computer\n",
      "🙂 - cable remote satellite\n",
      "🙂 - good sound\n",
      "😐 - stop work great\n",
      "🙂 - speaker best bose\n",
      "😐 - review beat price\n",
      "🙂 - play sound card\n",
      "🙂 - music movies watch\n",
      "🙂 - use easy quality\n",
      "😐 - buy worth excellent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "🙂 - bass sound good\n",
      "😐 - 10 psw\n",
      "😐 - better sub want\n",
      "🙂 - sound great\n",
      "🙂 - subwoofer input rca\n",
      "🙂 - live room\n",
      "🙂 - theater home\n",
      "😐 - price product quality\n",
      "😐 - port noise\n",
      "🙂 - speakers bookshelf polk\n",
      "😐 - music movies want\n",
      "🙂 - nice unit power\n",
      "🙂 - buy woofer surround\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "🙂 - fm station radio\n",
      "🙂 - price little great\n",
      "🙂 - fine work\n",
      "🙂 - batteries aa use\n",
      "🙂 - sound good quality\n",
      "😐 - mono headphones stereo\n",
      "😐 - good radio little\n",
      "😐 - control tune volume\n",
      "😐 - baseball listen game\n",
      "😐 - battery life\n",
      "😐 - sony radio pocket\n",
      "🙂 - buy love clear\n",
      "😐 - emergency case power\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "🙂 - music sound headphones\n",
      "🙂 - studio years home\n",
      "🙂 - ears head wear\n",
      "🙂 - sound comfortable great\n",
      "🙂 - review headphones say\n",
      "🙂 - response frequency price\n",
      "🙂 - work wear hours\n",
      "🙂 - fit large amp\n",
      "😐 - bass monitor bite\n",
      "🙂 - audio technica\n",
      "😐 - love far im\n",
      "🙂 - overall good better\n",
      "🙂 - hop record edit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "🙂 - speakers use speaker\n",
      "🙂 - z 5500\n",
      "🙂 - speakers price set\n",
      "🙂 - sound great\n",
      "😐 - day years ago\n",
      "🙂 - theater home\n",
      "🙂 - break customer send\n",
      "😐 - sound card\n",
      "🙂 - sound loud quality\n",
      "😐 - amaze buy clear\n",
      "😐 - promedia klipsch\n",
      "😐 - music listen game\n",
      "🙂 - recommend control highly panel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "🙂 - lens pen\n",
      "😐 - bag camera\n",
      "🙂 - lenses clean\n",
      "🙂 - dust brush end\n",
      "🙂 - use easy carry\n",
      "😐 - work product great\n",
      "🙂 - lens clean\n",
      "🙂 - good job\n",
      "😐 - use black time\n",
      "🙂 - lens dust filter\n",
      "😐 - cheap clean tool\n",
      "🙂 - kit clean camera\n",
      "😐 - handy buy friend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "🙂 - cable hdmi dvi\n",
      "😐 - cable price work\n",
      "😐 - laptop computer monitor\n",
      "😐 - good quality\n",
      "🙂 - work great\n",
      "😐 - pc tv connect\n",
      "😐 - xbox 360\n",
      "🙂 - perfect work monitor\n",
      "😐 - audio carry\n",
      "😐 - use ps3 problems\n",
      "😐 - review work product\n",
      "😐 - video card\n",
      "😐 - buy long quality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "🙂 - sound pair headphones\n",
      "😐 - good sound\n",
      "🙂 - price work great\n",
      "🙂 - cord use long\n",
      "😐 - recommend highly\n",
      "😐 - fit head phone\n",
      "🙂 - price low volume\n",
      "😐 - buy pair\n",
      "😐 - port lightweight xbs\n",
      "🙂 - use kid want\n",
      "😐 - player mp3\n",
      "😐 - ear bud\n",
      "😐 - light weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "🙂 - use headsets headset\n",
      "😐 - phone cordless\n",
      "🙂 - sound quality\n",
      "😐 - people hear clearly\n",
      "🙂 - price work great\n",
      "😐 - control volume\n",
      "😐 - fit head set\n",
      "🙂 - wear comfortable\n",
      "🙂 - free hand\n",
      "😐 - buy mm jack\n",
      "🙂 - work headset\n",
      "🙂 - use home phone\n",
      "😐 - break expect months\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "😐 - speaker use wire\n",
      "😐 - need install amp\n",
      "🙂 - cheap work great quality\n",
      "🙂 - watts rms\n",
      "🙂 - ship fast price amp\n",
      "😐 - thing watt price great\n",
      "🙂 - fine diameter work\n",
      "😐 - lead grind truck power\n",
      "🙂 - make clean easy amp\n",
      "🙂 - plenty wish wire\n",
      "😐 - product maybe u\n",
      "🙂 - wont work say\n",
      "😐 - help ice build\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "🙂 - player dvd\n",
      "😐 - play divx file\n",
      "🙂 - unit turn region\n",
      "🙂 - dvds play\n",
      "😕 - philips warranty months\n",
      "🙂 - price work great\n",
      "😕 - button hold stop close\n",
      "😐 - dvp 642\n",
      "🙂 - picture quality\n",
      "😐 - use composite component\n",
      "😐 - mpeg 4\n",
      "😐 - player remote work media\n",
      "🙂 - play good model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "😐 - cancel noise\n",
      "🙂 - sony headphones great\n",
      "😐 - good bose\n",
      "😐 - ears uncomfortable flight\n",
      "😐 - advertise work fragile\n",
      "😐 - return good pair\n",
      "😐 - reduce buy product\n",
      "😐 - noise cancellation sound\n",
      "🙂 - think dollar loud\n",
      "😐 - pad ear\n",
      "😐 - recommend hiss background headphones\n",
      "😐 - ones power flight\n",
      "😐 - order sony radio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "🙂 - dust blow\n",
      "😐 - air compress\n",
      "🙂 - work great\n",
      "😐 - bag camera\n",
      "🙂 - clean sensor\n",
      "😐 - air rocket blaster\n",
      "🙂 - blower nice quality\n",
      "😐 - lens dust remove\n",
      "😐 - use remove dust\n",
      "🙂 - air blast\n",
      "🙂 - buy good product\n",
      "🙂 - large work size\n",
      "🙂 - like look\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "🙂 - use mouse optical\n",
      "🙂 - mx1000 surface mx700\n",
      "😐 - button forward\n",
      "😐 - months close click\n",
      "🙂 - ergonomics w good\n",
      "😐 - mx revolution\n",
      "🙂 - mac easy set\n",
      "😐 - logitech mouse great\n",
      "😐 - years mouse time\n",
      "😐 - jump time long\n",
      "😐 - charge anymore usage\n",
      "😐 - office home\n",
      "😐 - use original actually\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "🙂 - speakers sound quality\n",
      "😐 - control turn volume\n",
      "🙂 - work great\n",
      "🙂 - use sound great\n",
      "🙂 - book mac pro sound\n",
      "😐 - save feature second\n",
      "🙂 - bass recommend highly\n",
      "😐 - amaze pay dollars\n",
      "😐 - harman kardon\n",
      "🙂 - happy purchase\n",
      "😐 - phone cell\n",
      "🙂 - speakers sound set\n",
      "🙂 - player cd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "🙂 - sound good sleeves\n",
      "🙂 - ear bud e3c\n",
      "🙂 - ears cord headphones\n",
      "😐 - ipod listen earphones\n",
      "😐 - break wire cover\n",
      "🙂 - love fit sound supply\n",
      "😐 - star price general\n",
      "🙂 - canal ear send\n",
      "🙂 - player e3cs purchase\n",
      "😐 - tip foam earplugs\n",
      "😐 - bass koss 10\n",
      "🙂 - hear long flight\n",
      "😐 - noise sound block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "🙂 - cord reel\n",
      "😐 - cable weight reel\n",
      "🙂 - hole end plug\n",
      "😐 - tie half fold\n",
      "😐 - hose use reel\n",
      "🙂 - buy product great\n",
      "😐 - heavy cord gauge\n",
      "😐 - use cord time\n",
      "😐 - foot cord 100\n",
      "😐 - cord extension 50\n",
      "😐 - cord hold extention\n",
      "😐 - like really look\n",
      "😐 - chord thrower bite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\nmf.py:1069: ConvergenceWarning: Maximum number of iteration 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\nmf.py:1069: ConvergenceWarning: Maximum number of iteration 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "😐 - buy pc car\n",
      "😐 - thing drum trick\n",
      "😐 - thing ipod brain say\n",
      "😐 - sound great\n",
      "😐 - complaints brain electronic\n",
      "😐 - sure shield bite static work\n",
      "😐 - cable audio headphones\n",
      "😐 - cable audio quality\n",
      "😐 - quality remote great\n",
      "😐 - sound distortion good\n",
      "😐 - cable sound defective\n",
      "😐 - suppose computer tv flat\n",
      "😐 - quite far bite static\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "😐 - player mp3 songs\n",
      "🙂 - use easy stop product\n",
      "🙂 - year hours pretty\n",
      "🙂 - player sound headphones\n",
      "🙂 - ergonomic better software device\n",
      "🙂 - 40 perfect expandable\n",
      "😐 - know sonic product blue\n",
      "😐 - practical unit version 256meg\n",
      "🙂 - music gb hours cali\n",
      "🙂 - useless love send\n",
      "😐 - rip cd\n",
      "😐 - mp3s computer car hell\n",
      "😐 - upgrader rio save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "🙂 - player sandisk good\n",
      "🙂 - use easy\n",
      "😐 - drive hard\n",
      "🙂 - memory expandable mp3\n",
      "🙂 - file crackle wma\n",
      "🙂 - amaze sweat years\n",
      "🙂 - include usb mp3\n",
      "🙂 - button buy small\n",
      "😐 - music classical solve volume\n",
      "🙂 - operation gift basic recommend product\n",
      "🙂 - point good review product\n",
      "🙂 - small ear easy v\n",
      "🙂 - audio track record\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "😐 - use regular mouse\n",
      "😐 - trackman marble wheel\n",
      "🙂 - version wire wireless\n",
      "😐 - track ball\n",
      "🙂 - button easy trackball\n",
      "🙂 - surface use\n",
      "🙂 - 5 years logitech\n",
      "🙂 - button thumb finger\n",
      "🙂 - love home game\n",
      "🙂 - laptop use work great\n",
      "🙂 - like feel model\n",
      "😐 - tunnel carpal great\n",
      "😐 - cord cordless device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "😐 - drive hard\n",
      "🙂 - support version work\n",
      "🙂 - external usb drive\n",
      "😐 - use ghost enclosure\n",
      "😐 - right box\n",
      "😐 - hdd computer dell\n",
      "🙂 - laptop upgrade ez\n",
      "😐 - way install connect\n",
      "🙂 - ssd minutes drive\n",
      "🙂 - like champ\n",
      "😐 - latitude time dell\n",
      "😐 - recovery space partition\n",
      "🙂 - recommend pc highly products\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "🙂 - use unit roadmate\n",
      "😐 - warranty extend\n",
      "😐 - update ask customer\n",
      "😐 - battery internal\n",
      "😐 - location save device\n",
      "😐 - loose fiancee door\n",
      "😐 - disappoint view excellent\n",
      "😐 - use freeway wife\n",
      "🙂 - newer sync gps\n",
      "😐 - 2004 speak model\n",
      "😐 - destination lexus cities\n",
      "😐 - minutes time months 10 5\n",
      "😐 - north interface america\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "😐 - radar detector\n",
      "🙂 - cord unit passport\n",
      "😐 - time cop drive\n",
      "😕 - service year heat\n",
      "😐 - 40 display cost\n",
      "😐 - expensive know protection\n",
      "😐 - mount suction windshield\n",
      "🙂 - better years old\n",
      "😐 - escort product website\n",
      "🙂 - relatively false positives car\n",
      "😐 - item recommend seller\n",
      "🙂 - x50 band units\n",
      "🙂 - far trust totally\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "🙂 - cord ipod headphones\n",
      "🙂 - buy sound earbuds\n",
      "🙂 - shure headphones quality\n",
      "😐 - coat wire expose\n",
      "😐 - bass buy sony\n",
      "😐 - long extension size\n",
      "😐 - use price\n",
      "🙂 - cable ear leave\n",
      "🙂 - house comfortably ear\n",
      "😐 - ears 5 completely\n",
      "😐 - bag music hear\n",
      "😐 - run say great\n",
      "😐 - player mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "🙂 - batteries charge charger\n",
      "😐 - display lcd\n",
      "😐 - use years work\n",
      "😐 - battery life\n",
      "😐 - super quick\n",
      "😐 - batteries days sony\n",
      "😐 - sony cells kill\n",
      "😐 - nice charge batteries\n",
      "😐 - battery know let charge 4\n",
      "😐 - fast charger\n",
      "😐 - camera digital\n",
      "🙂 - batteries faster great\n",
      "😐 - outlet plug wall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "🙂 - lens kit\n",
      "😐 - light stabilization zoom\n",
      "🙂 - print mm lense\n",
      "😐 - mm tamron canon\n",
      "😐 - repair canon problem\n",
      "🙂 - lens mm\n",
      "🙂 - shift return amazon\n",
      "🙂 - heavy picture big\n",
      "🙂 - 20d nice eos work great\n",
      "🙂 - lens money really\n",
      "🙂 - purpose general\n",
      "🙂 - price need worth f4l\n",
      "🙂 - lens good crisp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "🙂 - box jewel case\n",
      "🙂 - paper sleeves\n",
      "🙂 - need storage look\n",
      "🙂 - store really easy organize\n",
      "🙂 - easy assemble\n",
      "😐 - fold need store\n",
      "😐 - box snap lid\n",
      "🙂 - box dvd purchase\n",
      "🙂 - love use apart\n",
      "😐 - snap stay\n",
      "😐 - handy price product quality\n",
      "🙂 - box work\n",
      "🙂 - collection cd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "🙂 - lens use shoot\n",
      "😐 - f fringe l\n",
      "🙂 - f1 8\n",
      "🙂 - lens love\n",
      "🙂 - lens l purchase\n",
      "🙂 - nice image crisp\n",
      "😐 - light low\n",
      "🙂 - lens portraits great\n",
      "😐 - crop sensor\n",
      "🙂 - lens lenses make\n",
      "🙂 - sharp best fast\n",
      "😐 - 85mm ef canon\n",
      "🙂 - lens like recommend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "🙂 - shoot shots picture\n",
      "🙂 - thing like memory card\n",
      "😐 - paper machine quality\n",
      "😐 - photography camera snorkel\n",
      "🙂 - manual nice mode\n",
      "🙂 - think overall camera great\n",
      "🙂 - s100 fancy friends\n",
      "🙂 - thing usb easy\n",
      "😐 - battery backup photoshop\n",
      "🙂 - bother camera 2mp\n",
      "😐 - tft picture deal\n",
      "😐 - battery lithium charger\n",
      "😐 - reader small pocket goodbye\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "🙂 - turn cd mp3\n",
      "😐 - screw plastic work\n",
      "🙂 - good really sony\n",
      "😐 - use prevent trouble\n",
      "😐 - player mp3 rangeit far function\n",
      "😐 - 1 expensive 3\n",
      "😐 - player qualities purpose\n",
      "🙂 - listen walk book\n",
      "🙂 - 250 100 90\n",
      "🙂 - use optional disk accessory\n",
      "😐 - riovolt provide feature\n",
      "😕 - skip intuitive protection quite impressive\n",
      "🙂 - hope good qualitycons rat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "🙂 - like hd650 headphones\n",
      "🙂 - hd 650\n",
      "🙂 - sound hear headphones\n",
      "🙂 - headphone need sound\n",
      "😐 - tube hd600 amp\n",
      "😐 - ago years\n",
      "😐 - hd650 sound grado\n",
      "🙂 - watch tv\n",
      "🙂 - 650s hd\n",
      "🙂 - twice phone phenomenal\n",
      "🙂 - sound amplifier heck\n",
      "🙂 - dac end high\n",
      "🙂 - m100 wear design\n"
     ]
    }
   ],
   "source": [
    "#add emojis to topics\n",
    "import emoji\n",
    "for asin in valid_asins:\n",
    "    topics, reviews = print_topics(asin)\n",
    "\n",
    "    emoji_topics = []\n",
    "    for topic_id, topic in enumerate(topics):\n",
    "\n",
    "        # grab the average sentiment\n",
    "        product_reviews_df = pd.DataFrame(reviews)\n",
    "\n",
    "        product_reviews_df = product_reviews_df[product_reviews_df['topic'] == topic_id]\n",
    "        polarity   = product_reviews_df['summary_sentiment'].mean()\n",
    "\n",
    "        # append emojis to topic name based on range of sentiment\n",
    "        if polarity <= -0.5:\n",
    "            emoji_topics.append(\"😕 - \"+topic)\n",
    "        elif polarity > -0.5 and polarity < 0.5:\n",
    "            emoji_topics.append(\"😐 - \"+topic)\n",
    "        else:\n",
    "            emoji_topics.append(\"🙂 - \"+topic)\n",
    "\n",
    "    for emoji_topic in emoji_topics:\n",
    "        print(emoji.emojize(emoji_topic))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03445412464587205\n",
      "adjective and adverb count for Amazon scraped tags 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: 13\n",
      "0.03173139218403054\n",
      "adjective and adverb count for NMF 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivetha\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "0.07010330595238412\n",
      "adjective and adverb count for LDA 3\n"
     ]
    }
   ],
   "source": [
    "#string = 'focal length wide open full frame image quality depth of field great lens highly recommend autofocus purple fringing chromatic aberration build quality shallow depth background blur'\n",
    "string = 'sound quality ear canal bass response soudn isolation make sure noise cancelling much better every penny yellow foam worth every sony mdr-ex noise reduction ultimate ears'\n",
    "lemmas = tokenizeText(string) \n",
    "lemmas = ' '.join(lemma for lemma in lemmas)\n",
    "#print(lemmas)\n",
    "print(redundancy(lemmas))\n",
    "print(\"adjective and adverb count for Amazon scraped tags\", adjective_count(lemmas))\n",
    "\n",
    "topics, review = print_topics('B0001NNLHK')\n",
    "#print(topics)\n",
    "lemmas = ' '.join(lemma for lemma in topics)\n",
    "#print(lemmas)\n",
    "print(redundancy(lemmas))\n",
    "print(\"adjective and adverb count for NMF\", adjective_count(lemmas))\n",
    "\n",
    "topics, review = print_topics_lda('B0001NNLHK')\n",
    "#print(topics)\n",
    "lemmas = ' '.join(lemma for lemma in topics)\n",
    "#print(lemmas)\n",
    "print(redundancy(lemmas))\n",
    "print(\"adjective and adverb count for LDA\", adjective_count(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
